{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8a563f-a612-49c1-84bd-13d24180a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset,load_from_disk\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1df15d-ada5-4e7c-a9e7-cc5535ee3349",
   "metadata": {},
   "source": [
    "# load tokenizer and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4def262-84a4-44ee-b0af-ec7bd7ac1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='/project/lt200252-wcbart/nicky/cache_hug_1')\n",
    "aegis_2 = load_dataset(\n",
    "    \"json\",data_dir=\"/project/lt200252-wcbart/nicky/safety_dataset/nvidia/Aegis-AI-Content-Safety-Dataset-2.0/\",  data_files=[\"train.json\",\"refusals_train.json\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3de26f-5740-4c4b-9f9e-b2dc264b3f97",
   "metadata": {},
   "source": [
    "### count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8dfcfe-e125-4421-9a4f-b1571c670e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30007/30007 [00:09<00:00, 3139.09it/s]\n"
     ]
    }
   ],
   "source": [
    "total_vocab = []\n",
    "cnt = defaultdict(lambda : defaultdict(lambda:0)) \n",
    "cnt_bigram = defaultdict(lambda : defaultdict(lambda:0)) \n",
    "\n",
    "cnt_y = defaultdict(lambda :0)\n",
    "cnt_word = defaultdict(lambda :0)\n",
    "cnt_word_bigram = defaultdict(lambda :0)\n",
    "\n",
    "\n",
    "for x in tqdm(aegis_2['train']):\n",
    "    prompt_words = tokenizer(x['prompt'].strip())\n",
    "    ## unigram \n",
    "    for prompt_word in prompt_words['input_ids']:\n",
    "        cnt[str(x['prompt_label'])][prompt_word]+=1\n",
    "        cnt_word[prompt_word]+=1\n",
    "    ## bigram \n",
    "    for i in range(len(prompt_words['input_ids'])-1):\n",
    "        cnt_bigram[str(x['prompt_label'])][tuple([prompt_words['input_ids'][i],prompt_words['input_ids'][i+1]])]+=1\n",
    "        cnt_word_bigram[tuple([prompt_words['input_ids'][i],prompt_words['input_ids'][i+1]])]+=1\n",
    "        \n",
    "    cnt_y[str(x['prompt_label'])]+=1\n",
    "    total_vocab.extend(prompt_words['input_ids'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed632e0-ee18-4da3-bce5-20d5996f1e1f",
   "metadata": {},
   "source": [
    "### LMI (safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43540c43-c911-408f-bfb6-a645b26c6b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37840/37840 [00:00<00:00, 707527.03it/s]\n",
      "100%|██████████| 311357/311357 [00:04<00:00, 67570.44it/s]\n"
     ]
    }
   ],
   "source": [
    "LMIs = []\n",
    "D = len(set(total_vocab))\n",
    "T = len(total_vocab)\n",
    "p_Y = cnt_y['safe']/(cnt_y['unsafe']+ cnt_y['safe'])\n",
    "score_unharmful_LMIs = defaultdict(lambda : 0)\n",
    "score_unharmful_LMIs_bigram = defaultdict(lambda : 0)\n",
    "\n",
    "## unigram \n",
    "for idx,freq in tqdm(cnt['safe'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_unharmful_LMIs[idx] = float(LMI)\n",
    "## bigram \n",
    "for idx,freq in tqdm(cnt_bigram['safe'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word_bigram[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_unharmful_LMIs_bigram[tuple([tokenizer.decode(idx[0], skip_special_tokens=True),tokenizer.decode(idx[1], skip_special_tokens=True)])] = float(LMI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c9154-c9d9-4ca4-8228-35999105dd9f",
   "metadata": {},
   "source": [
    "### LMI (unsafe)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c66cfeb9-5cbe-4957-9157-2787a53fdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26003/26003 [00:00<00:00, 663340.69it/s]\n",
      "100%|██████████| 205982/205982 [00:03<00:00, 68107.21it/s]\n"
     ]
    }
   ],
   "source": [
    "LMIs = []\n",
    "D = len(set(total_vocab))\n",
    "T = len(total_vocab)\n",
    "p_Y = cnt_y['unsafe']/(cnt_y['unsafe']+ cnt_y['safe'])\n",
    "score_harmful_LMIs = defaultdict(lambda : 0)\n",
    "score_harmful_LMIs_bigram = defaultdict(lambda : 0)\n",
    "\n",
    "## unigram \n",
    "for idx,freq in tqdm(cnt['unsafe'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_harmful_LMIs[idx] = float(LMI)\n",
    "    \n",
    "## bigram \n",
    "for idx,freq in tqdm(cnt_bigram['unsafe'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word_bigram[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_harmful_LMIs_bigram[tuple([tokenizer.decode(idx[0], skip_special_tokens=True),tokenizer.decode(idx[1], skip_special_tokens=True)])] = float(LMI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ae8dc-49aa-48ef-bd11-7e5d706c0c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae19a57-4ca6-463b-818e-e7227141f3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb94e646-ad00-4c84-93cd-b39373043019",
   "metadata": {},
   "source": [
    "### head & tail distribution (unharmful) bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54097227-c75f-4869-bf37-dec90268a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score_unharmful_head_LMIs_bigram = sorted([ (k,v) for k,v in score_unharmful_LMIs_bigram.items()], key = lambda x: -x[1])\n",
    "sorted_unharmful_head_words_bigram = [k[0] for k in sorted_score_unharmful_head_LMIs_bigram][:100]\n",
    "# sorted_idx_harmful_head_words_bigram = [x[0] for x in sorted_score_harmful_head_LMIs_bigram][:100]\n",
    "\n",
    "\n",
    "sorted_score_unharmful_tail_LMIs_bigram = sorted([ (k,v) for k,v in score_unharmful_LMIs_bigram.items()], key = lambda x: x[1])\n",
    "sorted_unharmful_tail_words_bigram = [k[0] for k in sorted_score_unharmful_tail_LMIs_bigram][:100]\n",
    "# sorted_idx_harmful_tail_words_bigram = [x[0] for x in sorted_score_harmful_tail_LMIs_bigram][:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8471f1e-b4d2-4443-99b5-6ff463614b8f",
   "metadata": {},
   "source": [
    "### head & tail distribution (harmful) bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9a6df40-f83a-41ba-8b0c-e157563b84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score_harmful_head_LMIs_bigram = sorted([ (k,v) for k,v in score_harmful_LMIs_bigram.items()], key = lambda x: -x[1])\n",
    "sorted_harmful_head_words_bigram = [k[0] for k in sorted_score_harmful_head_LMIs_bigram][:100]\n",
    "sorted_idx_harmful_head_words_bigram = [x[0] for x in sorted_score_harmful_head_LMIs_bigram][:100]\n",
    "\n",
    "\n",
    "sorted_score_harmful_tail_LMIs_bigram = sorted([ (k,v) for k,v in score_harmful_LMIs_bigram.items()], key = lambda x: x[1])\n",
    "sorted_harmful_tail_words_bigram = [k[0] for k in sorted_score_harmful_tail_LMIs_bigram][:100]\n",
    "sorted_idx_harmful_tail_words_bigram = [x[0] for x in sorted_score_harmful_tail_LMIs_bigram][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74327ad5-734a-49e7-9376-07ebeac6766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('LMI_shortcut/tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_bigram_harmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_head_words_bigram,f)\n",
    "# with open('LMI_shortcut/tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_bigram_harmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_tail_words_bigram,f)\n",
    "\n",
    "# with open('LMI_shortcut/tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_bigram_unharmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_head_words_bigram,f)\n",
    "# with open('LMI_shortcut/tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_bigram_unharmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_tail_words_bigram,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e03f169d-5d0a-4f35-8f38-ff550cb964c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' how', ' to'),\n",
       " ('', 'How'),\n",
       " (' can', ' I'),\n",
       " (' to', ' get'),\n",
       " (' away', ' with'),\n",
       " (' get', ' away'),\n",
       " ('', 'I'),\n",
       " ('G', 'PT'),\n",
       " ('How', ' can'),\n",
       " (' Chat', 'G'),\n",
       " (' about', ' how'),\n",
       " (' I', \"'m\"),\n",
       " ('PT', ' with'),\n",
       " (' you', ' help'),\n",
       " ('I', \"'m\"),\n",
       " ('', 'What'),\n",
       " (' on', ' how'),\n",
       " ('.', ' Can'),\n",
       " (' want', ' to'),\n",
       " (' way', ' to'),\n",
       " (' I', ' can'),\n",
       " (' Can', ' you'),\n",
       " (' Open', 'AI'),\n",
       " (' Mode', ' enabled'),\n",
       " (' help', ' me'),\n",
       " ('', 'how'),\n",
       " ('.', ' It'),\n",
       " ('', 'Write'),\n",
       " (' and', ' I'),\n",
       " (' Developer', ' Mode'),\n",
       " (' do', ' I'),\n",
       " ('How', ' to'),\n",
       " (' to', ' steal'),\n",
       " (' address', ' for'),\n",
       " (' don', \"'t\"),\n",
       " ('.', ' DAN'),\n",
       " ('', 'Can'),\n",
       " (' do', ' anything'),\n",
       " (' trying', ' to'),\n",
       " (' are', ' some'),\n",
       " (' I', ' was'),\n",
       " ('Write', ' a'),\n",
       " ('how', ' to'),\n",
       " ('D', 'AN'),\n",
       " (' am', ' trying'),\n",
       " (' best', ' way'),\n",
       " ('How', ' do'),\n",
       " (' to', ' kill'),\n",
       " (' locate', ' the'),\n",
       " (' your', ' responses'),\n",
       " (' getting', ' caught'),\n",
       " (' residential', ' address'),\n",
       " (' someone', \"'s\"),\n",
       " ('.', ' He'),\n",
       " ('Can', ' you'),\n",
       " (' the', ' residential'),\n",
       " (' to', ' locate'),\n",
       " (' going', ' to'),\n",
       " (' to', ' do'),\n",
       " (' but', ' I'),\n",
       " ('.', ' Chat'),\n",
       " (' DAN', ','),\n",
       " (' can', \"'t\"),\n",
       " (' home', ' address'),\n",
       " (' to', ' commit'),\n",
       " (' I', ' don'),\n",
       " (\"'s\", ' the'),\n",
       " (' do', ' something'),\n",
       " (' without', ' getting'),\n",
       " (' with', ' Developer'),\n",
       " (' does', ' not'),\n",
       " (' me', '?'),\n",
       " ('I', ' am'),\n",
       " (' can', ' do'),\n",
       " (' so', ' I'),\n",
       " (' anything', ' now'),\n",
       " ('What', \"'s\"),\n",
       " (' none', ' of'),\n",
       " ('I', ' need'),\n",
       " (',', ' even'),\n",
       " (' DAN', ' can'),\n",
       " (' DAN', ' Mode'),\n",
       " (' I', \"'ve\"),\n",
       " (' insults', ' for'),\n",
       " (' in', ' character'),\n",
       " (' steal', ' a'),\n",
       " ('What', ' are'),\n",
       " ('', 'If'),\n",
       " (' responses', ' should'),\n",
       " (' the', ' home'),\n",
       " (' no', ' matter'),\n",
       " (',', ' I'),\n",
       " (' I', ' steal'),\n",
       " (' phone', ' number'),\n",
       " (' a', ' car'),\n",
       " (' some', ' insults'),\n",
       " (' as', ' Chat'),\n",
       " (' I', ' have'),\n",
       " (' a', ' DAN'),\n",
       " (' rob', ' a')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_harmful_head_words_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0ccb1-7a34-4183-81b5-38a92d37ac04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5ed1ca6-fcda-4f84-b5d7-79319499fc54",
   "metadata": {},
   "source": [
    "### head & tail distribution (harmful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5ef1b7a-5564-44ab-92ed-93d480b75f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score_harmful_head_LMIs = sorted([ (k,v) for k,v in score_harmful_LMIs.items()], key = lambda x: -x[1])\n",
    "# sorted_harmful_head_words = [tokenizer.convert_ids_to_tokens(x[0]) for x in sorted_score_harmful_head_LMIs][:100]\n",
    "sorted_harmful_head_words = [tokenizer.decode(x[0], skip_special_tokens=True) for x in sorted_score_harmful_head_LMIs][:100]\n",
    "sorted_idx_harmful_head_words = [x[0] for x in sorted_score_harmful_head_LMIs][:100]\n",
    "\n",
    "\n",
    "sorted_score_harmful_tail_LMIs = sorted([ (k,v) for k,v in score_harmful_LMIs.items()], key = lambda x: x[1])\n",
    "# sorted_harmful_tail_words = [tokenizer.convert_ids_to_tokens(x[0]) for x in sorted_score_harmful_tail_LMIs][:100]\n",
    "sorted_harmful_tail_words = [tokenizer.decode(x[0], skip_special_tokens=True) for x in sorted_score_harmful_tail_LMIs][:100]\n",
    "sorted_idx_harmful_tail_words = [x[0] for x in sorted_score_harmful_tail_LMIs][:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e594b02-53e0-4315-853e-6944304f1156",
   "metadata": {},
   "source": [
    "### head & tail distribution (unharmful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc3ec223-9bfe-4446-a6f1-b5379d0355a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score_unharmful_head_LMIs = sorted([ (k,v) for k,v in score_unharmful_LMIs.items()], key = lambda x: -x[1])\n",
    "sorted_unharmful_head_words = [tokenizer.decode(x[0], skip_special_tokens=True) for x in sorted_score_unharmful_head_LMIs][:100]\n",
    "sorted_idx_unharmful_head_words = [x[0] for x in sorted_score_unharmful_head_LMIs][:100]\n",
    "\n",
    "sorted_score_unharmful_tail_LMIs = sorted([ (k,v) for k,v in score_unharmful_LMIs.items()], key = lambda x: x[1])\n",
    "sorted_unharmful_tail_words = [tokenizer.decode(x[0], skip_special_tokens=True) for x in sorted_score_unharmful_tail_LMIs][:100]\n",
    "sorted_idx_unharmful_tail_words = [x[0] for x in sorted_score_unharmful_tail_LMIs][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7126327-2338-448b-aafb-19caeb3348c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2959d1b3-84f6-4f7e-b595-f559aa82a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_harmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_head_words,f)\n",
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_harmful_head_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_harmful_head_words,f)\n",
    "\n",
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_harmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_tail_words,f)\n",
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_harmful_tail_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_harmful_tail_words,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffffbb-6a4a-40ae-a240-e217c7f38b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d69a1-5057-4683-a172-728fefdb9582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3a4ca2a-4b4c-402d-b33d-f129426069c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_unharmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_head_words,f)\n",
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_unharmful_head_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_unharmful_head_words,f)\n",
    "\n",
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_unharmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_tail_words,f)\n",
    "# with open('tokenizer-meta-llama-Llama-3.1-8B-Instruct_dataset-Aegis2_LMI_unharmful_tail_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_unharmful_tail_words,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ab45d-fc69-445b-b1ab-87e5166d8d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
