{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8a563f-a612-49c1-84bd-13d24180a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset,load_from_disk\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords (only needed once)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cad444-7127-4c35-8937-000fdd227431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f1df15d-ada5-4e7c-a9e7-cc5535ee3349",
   "metadata": {},
   "source": [
    "# load tokenizer and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4def262-84a4-44ee-b0af-ec7bd7ac1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"allenai/wildguard\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='/project/lt200252-wcbart/nicky/cache_hug_1')\n",
    "wildguardtrain = load_from_disk(\"/project/lt200252-wcbart/nicky/safety_dataset/allenai/wildguardmix/wildguardtrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3de26f-5740-4c4b-9f9e-b2dc264b3f97",
   "metadata": {},
   "source": [
    "### count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8dfcfe-e125-4421-9a4f-b1571c670e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86759/86759 [00:37<00:00, 2300.26it/s]\n"
     ]
    }
   ],
   "source": [
    "total_vocab = []\n",
    "cnt = defaultdict(lambda : defaultdict(lambda:0)) \n",
    "cnt_bigram = defaultdict(lambda : defaultdict(lambda:0)) \n",
    "\n",
    "cnt_y = defaultdict(lambda :0)\n",
    "cnt_word = defaultdict(lambda :0)\n",
    "cnt_word_bigram = defaultdict(lambda :0)\n",
    "\n",
    "\n",
    "for x in tqdm(wildguardtrain['train']):\n",
    "    prompt_words = tokenizer(x['prompt'])\n",
    "    ## unigram \n",
    "    for prompt_word in prompt_words['input_ids']:\n",
    "        cnt[str(x['prompt_harm_label'])][prompt_word]+=1\n",
    "        cnt_word[prompt_word]+=1\n",
    "    ## bigram \n",
    "    for i in range(len(prompt_words['input_ids'])-1):\n",
    "        cnt_bigram[str(x['prompt_harm_label'])][tuple([prompt_words['input_ids'][i],prompt_words['input_ids'][i+1]])]+=1\n",
    "        cnt_word_bigram[tuple([prompt_words['input_ids'][i],prompt_words['input_ids'][i+1]])]+=1\n",
    "        \n",
    "    cnt_y[str(x['prompt_harm_label'])]+=1\n",
    "    total_vocab.extend(prompt_words['input_ids'])\n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed632e0-ee18-4da3-bce5-20d5996f1e1f",
   "metadata": {},
   "source": [
    "### LMI (unharmful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43540c43-c911-408f-bfb6-a645b26c6b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23301/23301 [00:00<00:00, 647477.03it/s]\n",
      "100%|██████████| 450896/450896 [00:01<00:00, 345222.27it/s]\n"
     ]
    }
   ],
   "source": [
    "LMIs = []\n",
    "D = len(set(total_vocab))\n",
    "T = len(total_vocab)\n",
    "p_Y = cnt_y['unharmful']/(cnt_y['unharmful']+ cnt_y['harmful'])\n",
    "score_unharmful_LMIs = defaultdict(lambda : 0)\n",
    "score_unharmful_LMIs_bigram = defaultdict(lambda : 0)\n",
    "\n",
    "## unigram \n",
    "for idx,freq in tqdm(cnt['unharmful'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_unharmful_LMIs[idx] = float(LMI)\n",
    "    \n",
    "## bigram \n",
    "for idx,freq in tqdm(cnt_bigram['unharmful'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word_bigram[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_unharmful_LMIs_bigram[tuple([tokenizer.convert_ids_to_tokens(idx[0]),tokenizer.convert_ids_to_tokens(idx[1])])] = float(LMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c9154-c9d9-4ca4-8228-35999105dd9f",
   "metadata": {},
   "source": [
    "### LMI (harmful) unigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c66cfeb9-5cbe-4957-9157-2787a53fdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19738/19738 [00:00<00:00, 652442.88it/s]\n",
      "100%|██████████| 427201/427201 [00:01<00:00, 358348.91it/s]\n"
     ]
    }
   ],
   "source": [
    "LMIs = []\n",
    "D = len(set(total_vocab))\n",
    "T = len(total_vocab)\n",
    "p_Y = cnt_y['harmful']/(cnt_y['unharmful']+ cnt_y['harmful'])\n",
    "score_harmful_LMIs = defaultdict(lambda : 0)\n",
    "score_harmful_LMIs_bigram = defaultdict(lambda : 0)\n",
    "\n",
    "## unigram \n",
    "for idx,freq in tqdm(cnt['harmful'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_harmful_LMIs[idx] = float(LMI)\n",
    "    \n",
    "## bigram \n",
    "for idx,freq in tqdm(cnt_bigram['harmful'].items()):\n",
    "    p_W_Y = freq/D\n",
    "    p_Y_W = freq/cnt_word_bigram[idx]\n",
    "    LMI = p_W_Y * np.log(p_Y_W/p_Y)\n",
    "    LMIs.append(float(LMI))\n",
    "    score_harmful_LMIs_bigram[tuple([tokenizer.convert_ids_to_tokens(idx[0]),tokenizer.convert_ids_to_tokens(idx[1])])] = float(LMI)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114592af-0649-45ec-b584-91d22c7f1b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2eb323a-5d6f-4e4b-8f06-f0107bc0790e",
   "metadata": {},
   "source": [
    "### head & tail distribution (harmful) bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9a6df40-f83a-41ba-8b0c-e157563b84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score_harmful_head_LMIs_bigram = sorted([ (k,v) for k,v in score_harmful_LMIs_bigram.items()], key = lambda x: -x[1])\n",
    "sorted_harmful_head_words_bigram = [k[0] for k in sorted_score_harmful_head_LMIs_bigram][:100]\n",
    "# sorted_idx_harmful_head_words_bigram = [x[0] for x in sorted_score_harmful_head_LMIs_bigram][:100]\n",
    "\n",
    "sorted_score_harmful_tail_LMIs_bigram = sorted([ (k,v) for k,v in score_harmful_LMIs_bigram.items()], key = lambda x: x[1])\n",
    "sorted_harmful_tail_words_bigram = [k[0] for k in sorted_score_harmful_tail_LMIs_bigram][:100]\n",
    "# sorted_idx_harmful_tail_words_bigram = [x[0] for x in sorted_score_harmful_tail_LMIs_bigram][:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc82a0c6-e576-403e-82bd-f2b9d18c7681",
   "metadata": {},
   "source": [
    "### head & tail distribution (unharmful) bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c68c028-9b74-4c3d-b158-6d0a7113f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score_unharmful_head_LMIs_bigram = sorted([ (k,v) for k,v in score_unharmful_LMIs_bigram.items()], key = lambda x: -x[1])\n",
    "sorted_unharmful_head_words_bigram = [k[0] for k in sorted_score_unharmful_head_LMIs_bigram][:100]\n",
    "# sorted_idx_harmful_head_words_bigram = [x[0] for x in sorted_score_harmful_head_LMIs_bigram][:100]\n",
    "\n",
    "\n",
    "sorted_score_unharmful_tail_LMIs_bigram = sorted([ (k,v) for k,v in score_unharmful_LMIs_bigram.items()], key = lambda x: x[1])\n",
    "sorted_unharmful_tail_words_bigram = [k[0] for k in sorted_score_unharmful_tail_LMIs_bigram][:100]\n",
    "# sorted_idx_harmful_tail_words_bigram = [x[0] for x in sorted_score_harmful_tail_LMIs_bigram][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8b78206-e9c9-406a-bc8a-26d069d9a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('LMI_shortcut/tokenizer-wildguard_dataset-wildguardmixTrain_LMI_bigram_harmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_head_words_bigram,f)\n",
    "# with open('LMI_shortcut/tokenizer-wildguard_dataset-wildguardmixTrain_LMI_bigram_harmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_tail_words_bigram,f)\n",
    "\n",
    "# with open('LMI_shortcut/tokenizer-wildguard_dataset-wildguardmixTrain_LMI_bigram_unharmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_head_words_bigram,f)\n",
    "# with open('LMI_shortcut/tokenizer-wildguard_dataset-wildguardmixTrain_LMI_bigram_unharmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_tail_words_bigram,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19b3cc-7461-4f77-952a-a7883d04ec18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5ed1ca6-fcda-4f84-b5d7-79319499fc54",
   "metadata": {},
   "source": [
    "### head & tail distribution (harmful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5ef1b7a-5564-44ab-92ed-93d480b75f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_score_harmful_head_LMIs = sorted([ (k,v) for k,v in score_harmful_LMIs.items()], key = lambda x: -x[1])\n",
    "# sorted_harmful_head_words = [tokenizer.convert_ids_to_tokens(x[0]) for x in sorted_score_harmful_head_LMIs][:100]\n",
    "# sorted_idx_harmful_head_words = [x[0] for x in sorted_score_harmful_head_LMIs][:100]\n",
    "\n",
    "\n",
    "# sorted_score_harmful_tail_LMIs = sorted([ (k,v) for k,v in score_harmful_LMIs.items()], key = lambda x: x[1])\n",
    "# sorted_harmful_tail_words = [tokenizer.convert_ids_to_tokens(x[0]) for x in sorted_score_harmful_tail_LMIs][:100]\n",
    "# sorted_idx_harmful_tail_words = [x[0] for x in sorted_score_harmful_tail_LMIs][:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e594b02-53e0-4315-853e-6944304f1156",
   "metadata": {},
   "source": [
    "### head & tail distribution (unharmful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3ec223-9bfe-4446-a6f1-b5379d0355a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_score_unharmful_head_LMIs = sorted([ (k,v) for k,v in score_unharmful_LMIs.items()], key = lambda x: -x[1])\n",
    "sorted_unharmful_head_words = [tokenizer.convert_ids_to_tokens(x[0]) for x in sorted_score_unharmful_head_LMIs][:100]\n",
    "sorted_idx_unharmful_head_words = [x[0] for x in sorted_score_unharmful_head_LMIs][:100]\n",
    "\n",
    "\n",
    "sorted_score_unharmful_tail_LMIs = sorted([ (k,v) for k,v in score_unharmful_LMIs.items()], key = lambda x: x[1])\n",
    "sorted_unharmful_tail_words = [tokenizer.convert_ids_to_tokens(x[0]) for x in sorted_score_unharmful_tail_LMIs][:100]\n",
    "sorted_idx_unharmful_tail_words = [x[0] for x in sorted_score_unharmful_tail_LMIs][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2959d1b3-84f6-4f7e-b595-f559aa82a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_harmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_head_words,f)\n",
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_harmful_head_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_harmful_head_words,f)\n",
    "\n",
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_harmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_harmful_tail_words,f)\n",
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_harmful_tail_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_harmful_tail_words,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80101d1-11f4-4dc9-be59-6ae165c7c54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?▁I▁What▁How'▁can▁game▁As▁'▁Can▁Imagine▁Why▁my<0x09>m▁some▁Could▁could▁AI▁do▁unique▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁strategies▁Remember▁in▁▁▁▁▁▁▁▁▁▁▁▁universe▁we▁world▁what▁You▁It▁Now▁magical▁This▁would▁=▁understanding▁explain▁concept▁telere▁novel▁our▁knowledge▁why▁realm▁Let▁creative▁creatures▁intr0C(ast▁en▁right▁journey▁self▁So▁respect▁protagon▁A▁mighten▁there▁copyright▁behind▁address▁▁▁▁▁▁▁▁approach▁insights▁find▁feelings▁ability',con▁beings;▁art▁young'.▁reasons▁hair▁learnport▁myth▁fantasy▁best▁help▁deep7▁player▁die▁emotional▁rights▁▁▁▁growth▁digital▁these\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(sorted_unharmful_head_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a4ca2a-4b4c-402d-b33d-f129426069c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_unharmful_head.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_head_words,f)\n",
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_unharmful_head_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_unharmful_head_words,f)\n",
    "\n",
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_unharmful_tail.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_unharmful_tail_words,f)\n",
    "# with open('tokenizer-wildguard_dataset-wildguardmixTrain_LMI_unharmful_tail_idx.pkl','wb') as f:\n",
    "#     pickle.dump(sorted_idx_unharmful_tail_words,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73f46a-7340-4581-8608-71e0c3d8d946",
   "metadata": {},
   "source": [
    "## example of shortcut keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90f06452-8f96-4341-a5ab-b599cbacf6cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I',\n",
       " '▁someone',\n",
       " 'harm',\n",
       " 'ly',\n",
       " 't',\n",
       " '▁self',\n",
       " '▁How',\n",
       " '▁can',\n",
       " \"'\",\n",
       " '▁Can',\n",
       " '▁sexual',\n",
       " '▁describe',\n",
       " '▁manip',\n",
       " '▁my',\n",
       " '▁sub',\n",
       " 's',\n",
       " 'leading',\n",
       " '▁without',\n",
       " '▁myself',\n",
       " '▁use',\n",
       " 'otic',\n",
       " '▁suggest',\n",
       " '▁er',\n",
       " '▁take',\n",
       " '▁involving',\n",
       " '▁way',\n",
       " '▁caught',\n",
       " '▁getting',\n",
       " '▁sex',\n",
       " '▁person',\n",
       " '▁racial',\n",
       " '▁effective',\n",
       " '▁work',\n",
       " '▁recommend',\n",
       " '▁Write',\n",
       " '▁cow',\n",
       " 'ork',\n",
       " '▁towards',\n",
       " '▁own',\n",
       " '▁group',\n",
       " 'atively',\n",
       " '▁de',\n",
       " '▁subtle',\n",
       " '▁colle',\n",
       " 'ague',\n",
       " '▁leaving',\n",
       " '▁partner',\n",
       " '▁opinion',\n",
       " '▁tips',\n",
       " '▁you',\n",
       " '▁neg',\n",
       " '▁progress',\n",
       " '▁detail',\n",
       " '▁fantasy',\n",
       " '▁mis',\n",
       " '▁Comp',\n",
       " '▁their',\n",
       " '▁pain',\n",
       " '▁won',\n",
       " '▁easy',\n",
       " '▁burning',\n",
       " '▁specific',\n",
       " '▁discre',\n",
       " '▁strategies',\n",
       " '▁detailed',\n",
       " '▁tactics',\n",
       " 'et',\n",
       " 'ose',\n",
       " '▁including',\n",
       " '▁share',\n",
       " '▁reputation',\n",
       " '▁method',\n",
       " '▁less',\n",
       " '▁porn',\n",
       " '▁people',\n",
       " '▁community',\n",
       " 'inder',\n",
       " '▁c',\n",
       " '▁obvious',\n",
       " 'ermine',\n",
       " '▁workplace',\n",
       " '▁productivity',\n",
       " '▁ri',\n",
       " 'ulate',\n",
       " '▁job',\n",
       " '▁good',\n",
       " 'cept',\n",
       " '▁ethnic',\n",
       " '▁give',\n",
       " '▁plan',\n",
       " '▁und',\n",
       " '▁career',\n",
       " '▁spread',\n",
       " '▁involves',\n",
       " '▁literature',\n",
       " '▁message',\n",
       " '▁details',\n",
       " '▁appears',\n",
       " '▁leave',\n",
       " 've']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_harmful_head_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88edae5f-c6aa-4e67-a216-00f0cec01c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '▁the',\n",
       " '▁of',\n",
       " '▁for',\n",
       " '▁a',\n",
       " '▁and',\n",
       " '▁are',\n",
       " '▁that',\n",
       " '▁in',\n",
       " '▁some',\n",
       " '▁while',\n",
       " '▁be',\n",
       " 'ing',\n",
       " '▁ensuring',\n",
       " '▁What',\n",
       " '▁harm',\n",
       " '▁legal',\n",
       " '▁any',\n",
       " '.',\n",
       " '▁not',\n",
       " '▁might',\n",
       " '▁but',\n",
       " '▁to',\n",
       " '▁or',\n",
       " '▁process',\n",
       " '▁activities',\n",
       " '▁used',\n",
       " '▁methods',\n",
       " '▁could',\n",
       " '▁is',\n",
       " '▁mim',\n",
       " '▁making',\n",
       " 'ical',\n",
       " '▁un',\n",
       " '▁purposes',\n",
       " '▁illegal',\n",
       " '▁access',\n",
       " '▁on',\n",
       " '▁Could',\n",
       " '▁as',\n",
       " '▁ethical',\n",
       " '▁sim',\n",
       " '▁with',\n",
       " '▁creating',\n",
       " '▁one',\n",
       " '▁potential',\n",
       " 'able',\n",
       " '▁from',\n",
       " '▁actions',\n",
       " 'ional',\n",
       " '▁explain',\n",
       " '▁by',\n",
       " '▁re',\n",
       " 'ot',\n",
       " '▁regulations',\n",
       " '▁scenario',\n",
       " '▁question',\n",
       " '▁guide',\n",
       " '▁laws',\n",
       " '▁information',\n",
       " '▁have',\n",
       " 'ics',\n",
       " '▁no',\n",
       " '▁actually',\n",
       " '▁techniques',\n",
       " 'het',\n",
       " '▁items',\n",
       " '▁hyp',\n",
       " '▁Expl',\n",
       " 'less',\n",
       " '▁how',\n",
       " '▁causing',\n",
       " '▁avoid',\n",
       " '▁program',\n",
       " '▁law',\n",
       " '▁may',\n",
       " '▁measures',\n",
       " '▁system',\n",
       " 'm',\n",
       " '▁conv',\n",
       " '▁when',\n",
       " '▁sure',\n",
       " '▁money',\n",
       " '▁all',\n",
       " '▁fict',\n",
       " '▁safety',\n",
       " 'ic',\n",
       " '▁behavior',\n",
       " '▁boundaries',\n",
       " '▁restricted',\n",
       " '▁if',\n",
       " 'stances',\n",
       " '▁within',\n",
       " '▁non',\n",
       " 'ful',\n",
       " 'incing',\n",
       " '▁appearance',\n",
       " 'izing',\n",
       " 'ates',\n",
       " '▁it']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_harmful_tail_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59d9bb91-16d1-4f42-b60d-b1ffb33cbca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '▁the',\n",
       " '▁of',\n",
       " '▁for',\n",
       " '▁and',\n",
       " '▁are',\n",
       " '▁a',\n",
       " '▁that',\n",
       " '▁purposes',\n",
       " '▁while',\n",
       " '▁legal',\n",
       " '▁ensuring',\n",
       " '▁harm',\n",
       " '▁in',\n",
       " 'less',\n",
       " '▁be',\n",
       " '▁some',\n",
       " '▁educational',\n",
       " 'ing',\n",
       " '▁program',\n",
       " '▁security',\n",
       " '▁any',\n",
       " '▁used',\n",
       " '▁might',\n",
       " '▁not',\n",
       " '▁process',\n",
       " '▁legally',\n",
       " '▁illegal',\n",
       " '▁sure',\n",
       " '.',\n",
       " '▁all',\n",
       " '▁ethical',\n",
       " '▁but',\n",
       " '▁sim',\n",
       " '▁What',\n",
       " 'het',\n",
       " 'ware',\n",
       " '▁activities',\n",
       " '▁mim',\n",
       " '▁hyp',\n",
       " '▁re',\n",
       " 'ical',\n",
       " '▁potential',\n",
       " '▁making',\n",
       " '▁could',\n",
       " 'ional',\n",
       " '▁laws',\n",
       " '▁avoid',\n",
       " '▁is',\n",
       " '▁building',\n",
       " '▁have',\n",
       " '▁pr',\n",
       " '▁controlled',\n",
       " '▁system',\n",
       " '▁safe',\n",
       " '▁mal',\n",
       " '▁respect',\n",
       " '▁Could',\n",
       " '▁access',\n",
       " '▁or',\n",
       " '▁creating',\n",
       " '▁to',\n",
       " '▁by',\n",
       " '▁methods',\n",
       " '▁only',\n",
       " '▁no',\n",
       " 'ank',\n",
       " '▁regulations',\n",
       " '▁law',\n",
       " '▁un',\n",
       " '▁test',\n",
       " '▁behavior',\n",
       " '▁may',\n",
       " '▁measures',\n",
       " '▁virus',\n",
       " '▁considered',\n",
       " '▁actions',\n",
       " '▁as',\n",
       " 'm',\n",
       " '▁actual',\n",
       " '▁should',\n",
       " '▁understand',\n",
       " '▁explain',\n",
       " '▁from',\n",
       " 'stances',\n",
       " '▁scenario',\n",
       " '▁r',\n",
       " 'om',\n",
       " 'able',\n",
       " 'plica',\n",
       " 'ans',\n",
       " '▁real',\n",
       " 'ically',\n",
       " '▁with',\n",
       " '▁fict',\n",
       " '▁safety',\n",
       " '▁mind',\n",
       " '▁one',\n",
       " 'ics',\n",
       " '▁on']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_unharmful_head_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "936c4f99-c153-41fc-910e-b13b1ee49331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I',\n",
       " '▁someone',\n",
       " '▁How',\n",
       " '▁can',\n",
       " \"'\",\n",
       " '▁Can',\n",
       " 'ly',\n",
       " 't',\n",
       " '▁self',\n",
       " 'harm',\n",
       " 's',\n",
       " '▁my',\n",
       " '▁without',\n",
       " '▁sub',\n",
       " '▁describe',\n",
       " '▁use',\n",
       " '▁manip',\n",
       " '▁way',\n",
       " '▁suggest',\n",
       " '▁sexual',\n",
       " '▁you',\n",
       " '▁person',\n",
       " '▁take',\n",
       " '▁Write',\n",
       " '▁their',\n",
       " '▁towards',\n",
       " '▁myself',\n",
       " '▁own',\n",
       " '▁group',\n",
       " '▁mis',\n",
       " '▁strategies',\n",
       " '▁involving',\n",
       " '▁work',\n",
       " '▁leaving',\n",
       " '▁specific',\n",
       " '▁discre',\n",
       " '▁racial',\n",
       " 'ulate',\n",
       " 'et',\n",
       " '▁recommend',\n",
       " '▁Comp',\n",
       " '▁de',\n",
       " '▁effective',\n",
       " '▁tactics',\n",
       " '▁people',\n",
       " 'leading',\n",
       " '▁neg',\n",
       " 'ose',\n",
       " '▁tips',\n",
       " '▁detailed',\n",
       " '▁cow',\n",
       " 'ork',\n",
       " '▁caught',\n",
       " 'atively',\n",
       " '▁ways',\n",
       " '▁plan',\n",
       " '▁give',\n",
       " '▁others',\n",
       " '▁provide',\n",
       " '▁negative',\n",
       " '▁share',\n",
       " '▁reputation',\n",
       " '▁including',\n",
       " '▁getting',\n",
       " '▁dis',\n",
       " '▁opinion',\n",
       " '▁partner',\n",
       " '▁appears',\n",
       " '▁details',\n",
       " '▁generate',\n",
       " '-',\n",
       " '▁make',\n",
       " '▁won',\n",
       " '▁pain',\n",
       " '▁message',\n",
       " '▁about',\n",
       " '▁community',\n",
       " '▁traces',\n",
       " '▁encounter',\n",
       " '▁content',\n",
       " '▁subtle',\n",
       " 'ity',\n",
       " '▁into',\n",
       " '▁against',\n",
       " '▁narrative',\n",
       " '▁internet',\n",
       " '▁presence',\n",
       " '▁job',\n",
       " 'cept',\n",
       " '▁good',\n",
       " '▁ethnic',\n",
       " 'er',\n",
       " '▁seem',\n",
       " '▁Describe',\n",
       " 'raft',\n",
       " '▁story',\n",
       " '▁between',\n",
       " '▁up',\n",
       " '▁campaign',\n",
       " '▁following']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_unharmful_tail_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049e8db-d866-41d2-b6b5-966cea9cebd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
